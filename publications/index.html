<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Research | Bo Ai </title> <meta name="author" content="Bo Ai"> <meta name="description" content="Robotics, Machine Learning"> <meta name="keywords" content="robotics, machine learning, artificial intelligence"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ucsd-logo.png?ccd154eecfbaa8871c35216e4979e7b5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://boai01.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Bo Ai </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Research</h1> <p class="post-description">Robotics, Machine Learning</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025ScienceRobotics-480.webp 480w,/assets/img/publication_preview/2025ScienceRobotics-800.webp 800w,/assets/img/publication_preview/2025ScienceRobotics-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2025ScienceRobotics.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025ScienceRobotics.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ai2025review" class="col-sm-8"> <div class="title">A Review of Learning-Based Dynamics Models for Robotic Manipulation</div> <div class="author"> <strong>Bo Ai</strong>, <a href="https://s-tian.github.io/" rel="external nofollow noopener" target="_blank">Stephen Tian</a>, <a href="https://hshi74.github.io/" rel="external nofollow noopener" target="_blank">Haochen Shi</a>, <a href="https://wangyixuan12.github.io/" rel="external nofollow noopener" target="_blank">Yixuan Wang</a>, <a href="https://tobiaspfaff.com/" rel="external nofollow noopener" target="_blank">Tobias Pfaff</a>, <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan" rel="external nofollow noopener" target="_blank">Cheston Tan</a>, <a href="https://hichristensen.com/" rel="external nofollow noopener" target="_blank">Henrik I. Christensen</a>, <a href="https://cseweb.ucsd.edu/~haosu/index.html#_ri" rel="external nofollow noopener" target="_blank">Hao Su</a>, <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a>, and <a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a> </div> <div class="periodical"> <em>Science Robotics</em>, 2025 </div> <div class="small-note"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.science.org/stoken/author-tokens/ST-2903/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://www.science.org/stoken/author-tokens/ST-2903/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Dynamics models that predict the effects of physical interactions are essential for planning and control in robotic manipulation. Although models based on physical principles often generalize well, they typically require full-state information, which can be difficult or impossible to extract from perception data in complex, real-world scenarios. Learning-based dynamics models provide an alternative by deriving state transition functions purely from perceived interaction data, enabling the capture of complex, hard-to-model factors and predictive uncertainty and accelerating simulations that are often too slow for real-time control. Recent successes in this field have demonstrated notable advancements in robot capabilities, including long-horizon manipulation of deformable objects, granular materials, and complex multiobject interactions such as stowing and packing. A crucial aspect of these investigations is the choice of state representation, which determines the inductive biases in the learning system for reduced-order modeling of scene dynamics. This article provides a timely and comprehensive review of current techniques and trade-offs in designing learned dynamics models, highlighting their role in advancing robot capabilities through integration with state estimation and control and identifying critical research gaps for future exploration. Dynamics models learned from real-world interactions with task-aligned representations empower robotic manipulation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025CoRL-ESL.webp-480.webp 480w,/assets/img/publication_preview/2025CoRL-ESL.webp-800.webp 800w,/assets/img/publication_preview/2025CoRL-ESL.webp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2025CoRL-ESL.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025CoRL-ESL.webp" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ai2025embodimentscalinglaws" class="col-sm-8"> <div class="title">Towards Embodiment Scaling Laws in Robot Locomotion</div> <div class="author"> <strong>Bo Ai*</strong>, <a href="https://liudai.notion.site/Liu-DAI-429d262c9f504ef59a5668772a29e86b" rel="external nofollow noopener" target="_blank">Liu Dai*</a>, <a href="https://www.ias.informatik.tu-darmstadt.de/Team/NicoBohlinger" rel="external nofollow noopener" target="_blank">Nico Bohlinger*</a> , Dichen Li*, <a href="https://cseweb.ucsd.edu/~t3mu/" rel="external nofollow noopener" target="_blank">Tongzhou Mu</a> , <a href="https://zhanxinwu.com/" rel="external nofollow noopener" target="_blank">Zhanxin Wu</a>, K. Fay, <a href="https://hichristensen.com/" rel="external nofollow noopener" target="_blank">Henrik I. Christensen</a>, <a href="https://www.ias.tu-darmstadt.de/Team/JanPeters" rel="external nofollow noopener" target="_blank">Jan Peters</a>, and <a href="https://cseweb.ucsd.edu/~haosu/index.html#_ri" rel="external nofollow noopener" target="_blank">Hao Su</a> </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL)</em>, 2025 </div> <div class="small-note"> Abridged in RSS 2025 workshop on <a href="https://rss-hardware-intelligence.github.io/" rel="external nofollow noopener" target="_blank">Hardware-Aware Intelligence</a> and CoRL 2025 workshop on <a href="https://sites.google.com/stanford.edu/corldata25/home" rel="external nofollow noopener" target="_blank">Robot Data</a>. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://embodiment-scaling-laws.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://arxiv.org/abs/2505.05753" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Cross-embodiment generalization underpins the vision of building generalist embodied agents for <em>any</em> robot, yet its enabling factors remain poorly understood. We investigate <em>embodiment scaling laws</em>, the hypothesis that increasing the number of training embodiments improves generalization to unseen ones, using robot locomotion as a test bed. We procedurally generate approximately 1,000 embodiments with topological, geometric, and joint-level kinematic variations, and train policies on random subsets. We observe positive scaling trends supporting the hypothesis, and find that embodiment scaling enables substantially broader generalization than data scaling on fixed embodiments. Our best policy, trained on the full dataset, transfers zero-shot to novel embodiments in simulation and the real world, including the Unitree Go2 and H1. These results represent a step toward general embodied intelligence, with relevance to adaptive control for configurable robots, morphology–control co-design, and beyond. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025CoRL-SAVOR.webp-480.webp 480w,/assets/img/publication_preview/2025CoRL-SAVOR.webp-800.webp 800w,/assets/img/publication_preview/2025CoRL-SAVOR.webp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2025CoRL-SAVOR.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025CoRL-SAVOR.webp" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wu2025savorskillaffordancelearning" class="col-sm-8"> <div class="title">SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition</div> <div class="author"> <a href="https://zhanxinwu.com/" rel="external nofollow noopener" target="_blank">Zhanxin Wu</a>, <strong>Bo Ai</strong>, <a href="https://tomsilver.github.io/" rel="external nofollow noopener" target="_blank">Tom Silver</a>, and <a href="https://sites.google.com/site/tapomayukh" rel="external nofollow noopener" target="_blank">Tapomayukh Bhattacharjee</a> </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL)</em>, 2025 </div> <div class="small-note"> Oral Presentation </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://emprise.cs.cornell.edu/savor/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://arxiv.org/abs/2506.02353" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Robot-assisted feeding requires reliable bite acquisition, a challenging task due to the complex interactions between utensils and food with diverse physical properties. These interactions are further complicated by the temporal variability of food properties—for example, steak becomes firm as it cools even during a meal. To address this, we propose SAVOR, a novel approach for learning skill affordances for bite acquisition—how suitable a manipulation skill (e.g., skewering, scooping) is for a given utensil-food interaction. In our formulation, skill affordances arise from the combination of tool affordances (what a utensil can do) and food affordances (what the food allows). Tool affordances are learned offline through calibration, where different utensils interact with a variety of foods to model their functional capabilities. Food affordances are characterized by physical properties such as softness, moisture, and viscosity, initially inferred through commonsense reasoning using a visually-conditioned language model and then dynamically refined through online visuo-haptic perception using SAVOR during interaction. Our method integrates these offline and online estimates to predict skill affordances in real time, enabling the robot to select the most appropriate skill for each food item. Evaluated on 20 single-item foods and 10 in-the-wild meals, our approach improves bite acquisition success by 13% over state-of-the-art (SOTA) category-based methods (e.g. use skewer for fruits). These results highlight the importance of modeling interaction-driven skill affordances for generalizable and effective robot-assisted bite acquisition.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025CoRL-UniClothDiff.webp-480.webp 480w,/assets/img/publication_preview/2025CoRL-UniClothDiff.webp-800.webp 800w,/assets/img/publication_preview/2025CoRL-UniClothDiff.webp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2025CoRL-UniClothDiff.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025CoRL-UniClothDiff.webp" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tian2025diffusion" class="col-sm-8"> <div class="title">Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation</div> <div class="author"> Tongxuan Tian* , Haoyang Li*, <strong>Bo Ai</strong>, <a href="https://scholar.google.com/citations?view_op=list_works&amp;hl=en&amp;user=i-QiNPIAAAAJ" rel="external nofollow noopener" target="_blank">Xiaodi Yuan</a>, <a href="https://sites.google.com/view/zhiao-huang" rel="external nofollow noopener" target="_blank">Zhiao Huang</a>, and <a href="https://cseweb.ucsd.edu/~haosu/index.html#_ri" rel="external nofollow noopener" target="_blank">Hao Su</a> </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL)</em>, 2025 </div> <div class="small-note"> Abridged in RSS 2025 workshop on <a href="https://swomo-rss.github.io/" rel="external nofollow noopener" target="_blank">Structured World Models for Robotic Manipulation</a> and CoRL 2025 workshop on <a href="https://simulatingrobotworlds.github.io/" rel="external nofollow noopener" target="_blank">Learning to Simulate Robot Worlds</a>. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://uniclothdiff.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://arxiv.org/abs/2503.11999" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Manipulating deformable objects like cloth is challenging due to their complex dynamics, near-infinite degrees of freedom, and frequent self-occlusions, which complicate state estimation and dynamics modeling. Prior work has struggled with robust cloth state estimation, while dynamics models, primarily based on Graph Neural Networks (GNNs), are limited by their locality. Inspired by recent advances in generative models, we hypothesize that these expressive models can effectively capture intricate cloth configurations and deformation patterns from data. Building on this insight, we propose a diffusion-based generative approach for both perception and dynamics modeling. Specifically, we formulate state estimation as reconstructing the full cloth state from sparse RGB-D observations conditioned on a canonical cloth mesh and dynamics modeling as predicting future states given the current state and robot actions. Leveraging a transformer-based diffusion model, our method achieves high-fidelity state reconstruction while reducing long-horizon dynamics prediction errors by an order of magnitude compared to GNN-based approaches. Integrated with model-predictive control (MPC), our framework successfully executes cloth folding on a real robotic system, demonstrating the potential of generative models for manipulation tasks with partial observability and complex dynamics.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025ARR-WMABench-480.webp 480w,/assets/img/publication_preview/2025ARR-WMABench-800.webp 800w,/assets/img/publication_preview/2025ARR-WMABench-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2025ARR-WMABench.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025ARR-WMABench.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2025do" class="col-sm-8"> <div class="title">Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation</div> <div class="author"> <a href="https://scholar.google.com/citations?user=-zquUaEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Qiyue Gao*</a>, <a href="https://scholar.google.com/citations?user=UPtuhT4AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xinyu Pi*</a> , Kevin Liu, Junrong Chen, Ruolan Yang , Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, <strong>Bo Ai</strong>, <a href="https://www.stoneztao.com/" rel="external nofollow noopener" target="_blank">Stone Tao</a> , Mengyang Liu, Jiaxi Yang, Chao-Jung Lai, Chuanyang Jin, Jiannan Xiang , Benhao Huang, <a href="https://www.daviddanks.org/" rel="external nofollow noopener" target="_blank">David Danks</a>, <a href="https://cseweb.ucsd.edu/~haosu/index.html#_ri" rel="external nofollow noopener" target="_blank">Hao Su</a>, <a href="https://www.tshu.io/" rel="external nofollow noopener" target="_blank">Tianmin Shu</a>, Ziqiao Ma , <a href="https://lianhui.ucsd.edu/" rel="external nofollow noopener" target="_blank">Lianhui Qin</a>, and <a href="https://zhiting.ucsd.edu/" rel="external nofollow noopener" target="_blank">Zhiting Hu</a> </div> <div class="periodical"> <em>ACL Findings</em> , 2025 </div> <div class="small-note"> Abridged in ICLR 2025 workshop on <a href="https://sites.google.com/view/worldmodel-iclr2025/home" rel="external nofollow noopener" target="_blank">World Models: Understanding, Modelling, and Scaling</a>. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://wm-abench.maitrix.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://arxiv.org/abs/2506.21876" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Internal world models (WMs) enable agents to understand the world’s state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs’ fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 517 controlled experiments on 11 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding—e.g., they tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025IROS-AdaDexGrasp.webp-480.webp 480w,/assets/img/publication_preview/2025IROS-AdaDexGrasp.webp-800.webp 800w,/assets/img/publication_preview/2025IROS-AdaDexGrasp.webp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2025IROS-AdaDexGrasp.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025IROS-AdaDexGrasp.webp" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shi2025learningadaptivedexterousgrasping" class="col-sm-8"> <div class="title">Learning Adaptive Dexterous Grasping from Single Demonstrations</div> <div class="author"> Liangzhi Shi*, <a href="https://liuyulinn.github.io/" rel="external nofollow noopener" target="_blank">Yulin Liu*</a>, Lingqi Zeng*, <strong>Bo Ai</strong>, Zhengdong Hong, and <a href="https://cseweb.ucsd.edu/~haosu/index.html#_ri" rel="external nofollow noopener" target="_blank">Hao Su</a> </div> <div class="periodical"> <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2025 </div> <div class="small-note"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://zenglingqi647.github.io/AdaDexGrasp/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://arxiv.org/abs/2503.20208" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>How can robots learn dexterous grasping skills efficiently and apply them adaptively based on user instructions? This work tackles two key challenges: efficient skill acquisition from limited human demonstrations and context-driven skill selection. We introduce AdaDexGrasp, a framework that learns a library of grasping skills from a single human demonstration per skill and selects the most suitable one using a vision-language model (VLM). To improve sample efficiency, we propose a trajectory following reward that guides reinforcement learning (RL) toward states close to a human demonstration while allowing flexibility in exploration. To learn beyond the single demonstration, we employ curriculum learning, progressively increasing object pose variations to enhance robustness. At deployment, a VLM retrieves the appropriate skill based on user instructions, bridging low-level learned skills with high-level intent. We evaluate AdaDexGrasp in both simulation and real-world settings, showing that our approach significantly improves RL efficiency and enables learning human-like grasp strategies across varied object configurations. Finally, we demonstrate zero-shot transfer of our learned policies to a real-world PSYONIC Ability Hand, with a 90% success rate across objects, significantly outperforming the baseline.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024IJRR-kilo-nav-15x-480p.webp-480.webp 480w,/assets/img/publication_preview/2024IJRR-kilo-nav-15x-480p.webp-800.webp 800w,/assets/img/publication_preview/2024IJRR-kilo-nav-15x-480p.webp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2024IJRR-kilo-nav-15x-480p.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024IJRR-kilo-nav-15x-480p.webp" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2024intentionnet" class="col-sm-8"> <div class="title">IntentionNet: Map-Lite Visual Navigation at the Kilometre Scale</div> <div class="author"> Wei Gao, <strong>Bo Ai</strong>, Joel Loo,  Vinay, and <a href="https://www.comp.nus.edu.sg/~dyhsu/" rel="external nofollow noopener" target="_blank">David Hsu</a> </div> <div class="periodical"> <em>arXiv</em>, 2024 </div> <div class="small-note"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2025arxiv-intention-net.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This work explores the challenges of creating a scalable and robust robot navigation system that can traverse both indoor and outdoor environments to reach distant goals. We propose a navigation system architecture called IntentionNet that employs a monolithic neural network as the low-level planner/controller, and uses a general interface that we call intentions to steer the controller. The paper proposes two types of intentions, Local Path and Environment (LPE) and Discretised Local Move (DLM), and shows that DLM is robust to significant metric positioning and mapping errors. The paper also presents Kilo-IntentionNet, an instance of the IntentionNet system using the DLM intention that is deployed on a Boston Dynamics Spot robot, and which successfully navigates through complex indoor and outdoor environments over distances of up to a kilometre with only noisy odometry.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024EMNLP-GMeLLo-480.webp 480w,/assets/img/publication_preview/2024EMNLP-GMeLLo-800.webp 800w,/assets/img/publication_preview/2024EMNLP-GMeLLo-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2024EMNLP-GMeLLo.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024EMNLP-GMeLLo.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/corr/abs-2408-15903" class="col-sm-8"> <div class="title">LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments</div> <div class="author"> Ruirui Chen, Weifeng Jiang, <a href="https://qcwthu.github.io/" rel="external nofollow noopener" target="_blank">Chengwei Qin</a>, Ishaan Singh Rawal, <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan" rel="external nofollow noopener" target="_blank">Cheston Tan</a>, <a href="http://www.dongkyu.com/" rel="external nofollow noopener" target="_blank">Dongkyu Choi</a>, <a href="https://boxiong.io/" rel="external nofollow noopener" target="_blank">Bo Xiong</a>, and <strong>Bo Ai</strong> </div> <div class="periodical"> <em>EMNLP Findings</em>, 2024 </div> <div class="small-note"> </div> <div class="links"> <a href="https://doi.org/10.48550/arXiv.2408.15903" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024RSS-RoboPack.webp-480.webp 480w,/assets/img/publication_preview/2024RSS-RoboPack.webp-800.webp 800w,/assets/img/publication_preview/2024RSS-RoboPack.webp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2024RSS-RoboPack.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024RSS-RoboPack.webp" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ai2024robopack" class="col-sm-8"> <div class="title">RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing</div> <div class="author"> <strong>Bo Ai*</strong>, <a href="https://s-tian.github.io/" rel="external nofollow noopener" target="_blank">Stephen Tian*</a>, <a href="https://hshi74.github.io/" rel="external nofollow noopener" target="_blank">Haochen Shi</a>, <a href="https://wangyixuan12.github.io/" rel="external nofollow noopener" target="_blank">Yixuan Wang</a>, <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan" rel="external nofollow noopener" target="_blank">Cheston Tan</a>, <a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Robotics: Science and Systems (RSS)</em> , 2024 </div> <div class="small-note"> Abridged in ICRA 2024 workshops <a href="https://shanluo.github.io/ViTacWorkshops/" rel="external nofollow noopener" target="_blank">ViTac</a>, <a href="https://3d-manipulation-workshop.github.io/" rel="external nofollow noopener" target="_blank">3DVRM</a>, <a href="https://icra-manipulation-skill.github.io/" rel="external nofollow noopener" target="_blank">Future Roadmap for Sensorimotor Skills</a>, and RSS 2024 workshop <a href="https://sites.google.com/alora.tech/priors4robots24" rel="external nofollow noopener" target="_blank">Priors4Robots</a>. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://robo-pack.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://arxiv.org/abs/2407.01418" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ai2024robopack</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ai*, Bo and Tian*, Stephen and Shi, Haochen and Wang, Yixuan and Tan, Cheston and Li, Yunzhu and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics: Science and Systems (RSS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2407.01418}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Abridged in ICRA 2024 workshops
    [ViTac](https://shanluo.github.io/ViTacWorkshops/),
    [3DVRM](https://3d-manipulation-workshop.github.io/),
    [Future Roadmap for Sensorimotor Skills](https://icra-manipulation-skill.github.io/), and RSS 2024 workshop
    [Priors4Robots](https://sites.google.com/alora.tech/priors4robots24).}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023ISER-SEER.webp-480.webp 480w,/assets/img/publication_preview/2023ISER-SEER.webp-800.webp 800w,/assets/img/publication_preview/2023ISER-SEER.webp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2023ISER-SEER.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023ISER-SEER.webp" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ai2023invariance" class="col-sm-8"> <div class="title">Invariance is Key to Generalization: Examining the Role of Representation in Sim-to-Real Transfer for Visual Navigation</div> <div class="author"> <strong>Bo Ai</strong> , <a href="https://zhanxinwu.com/" rel="external nofollow noopener" target="_blank">Zhanxin Wu</a>, and <a href="https://www.comp.nus.edu.sg/~dyhsu/" rel="external nofollow noopener" target="_blank">David Hsu</a> </div> <div class="periodical"> <em>International Symposium on Experimental Robotics (ISER)</em> , 2023 </div> <div class="small-note"> Published within <a href="https://link.springer.com/chapter/10.1007/978-3-031-63596-0_7" rel="external nofollow noopener" target="_blank">Springer Proceedings in Advanced Robotics (SPAR)</a>. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2310.15020" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The data-driven approach to robot control has been gathering pace rapidly, yet generalization to unseen task domains remains a critical challenge. We argue that the key to generalization is representations that are (i) rich enough to capture all task-relevant information and (ii) invariant to superfluous variability between the training and the test domains. We experimentally study such a representation—containing both depth and semantic information—for visual navigation and show that it enables a control policy trained entirely in simulated indoor scenes to generalize to diverse real-world environments, both indoors and outdoors. Further, we show that our representation reduces the A-distance between the training and test domains, improving the generalization error bound as a result. Our proposed approach is scalable: the learned policy improves continuously, as the foundation models that it exploits absorb more diverse data during pre-training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023RSSW-TAMP.webp-480.webp 480w,/assets/img/publication_preview/2023RSSW-TAMP.webp-800.webp 800w,/assets/img/publication_preview/2023RSSW-TAMP.webp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2023RSSW-TAMP.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023RSSW-TAMP.webp" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wu2023integrating" class="col-sm-8"> <div class="title">Integrating Common Sense and Planning with Large Language Models for Room Tidying</div> <div class="author"> <a href="https://zhanxinwu.com/" rel="external nofollow noopener" target="_blank">Zhanxin Wu</a>, <strong>Bo Ai</strong>, and <a href="https://www.comp.nus.edu.sg/~dyhsu/" rel="external nofollow noopener" target="_blank">David Hsu</a> </div> <div class="periodical"> <em>RSS 2023 Workshop on Learning for Task and Motion Planning</em> , 2023 </div> <div class="small-note"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=vuSI9mhDaBZ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Do you want a personal housekeeper robot? This project seeks to endow robots with the capability of tidying up messy rooms with brief natural language descriptions of the environment. We address three key challenges: (i) incomplete map information in the description, (ii) commonsense understanding of object locations, and (iii) long-horizon planning and acting to achieve the objective. To tackle these challenges, we leverage Large Language Models’ (LLMs) understanding of typical layouts of human-living environments and object locations, as well as programming and control skills for action execution. Specifically, we prompt ChatGPT to reconstruct complete map representations from partial descriptions, then generate a high-level action plan in the form of Python functions, and finally refine the plans with atomic actions executable by the robot. We show that our framework enables effective room rearrangement with limited human instruction guidance. On simulation and real-world maps, it is able to find a place missing out from human description within three interactions with humans. In the simulation environment, it is capable of putting more than 80% household objects in their desired place. This study provides preliminary evidence that LLMs have common sense about the spatial layout of human-living environments and object arrangements, and this work connects this knowledge to robotics tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2023integrating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Integrating Common Sense and Planning with Large Language Models for Room Tidying}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zhanxin and Ai, Bo and Hsu, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{RSS 2023 Workshop on Learning for Task and Motion Planning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=vuSI9mhDaBZ}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2022ICRA-DECISION.webp-480.webp 480w,/assets/img/publication_preview/2022ICRA-DECISION.webp-800.webp 800w,/assets/img/publication_preview/2022ICRA-DECISION.webp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2022ICRA-DECISION.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022ICRA-DECISION.webp" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ai2022deep" class="col-sm-8"> <div class="title">Deep Visual Navigation under Partial Observability</div> <div class="author"> <strong>Bo Ai</strong> , Wei Gao,  Vinay, and <a href="https://www.comp.nus.edu.sg/~dyhsu/" rel="external nofollow noopener" target="_blank">David Hsu</a> </div> <div class="periodical"> <em>International Conference on Robotics and Automation (ICRA)</em> , 2022 </div> <div class="small-note"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://adacomp.comp.nus.edu.sg/inet/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://arxiv.org/abs/2109.07752" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>How can a robot navigate successfully in rich and diverse environments, indoors or outdoors, along office corridors or trails on the grassland, on the flat ground or the staircase? To this end, this work aims to address three challenges: (i) complex visual observations, (ii) partial observability of local visual sensing, and (iii) multimodal robot behaviors conditioned on both the local environment and the global navigation objective. We propose to train a neural network (NN) controller for local navigation via imitation learning. To tackle complex visual observations, we extract multi-scale spatial representations through CNNs. To tackle partial observability, we aggregate multi-scale spatial information over time and encode it in LSTMs. To learn multimodal behaviors, we use a separate memory module for each behavior mode. Importantly, we integrate the multiple neural network modules into a unified controller that achieves robust performance for visual navigation in complex, partially observable environments. We implemented the controller on the quadrupedal Spot robot and evaluated it on three challenging tasks: adversarial pedestrian avoidance, blind-spot obstacle avoidance, and elevator riding. The experiments show that the proposed NN architecture significantly improves navigation performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ai2022deep</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ai, Bo and Gao, Wei and Vinay and Hsu, David}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Visual Navigation under Partial Observability}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9439--9446}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICRA46639.2022.9811598}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA46639.2022.9811598}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Mon, 04 Dec 2023 21:29:46 +0100}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/icra/AiGVH22.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2022IJCNLP-Contrax-480.webp 480w,/assets/img/publication_preview/2022IJCNLP-Contrax-800.webp 800w,/assets/img/publication_preview/2022IJCNLP-Contrax-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2022IJCNLP-Contrax.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022IJCNLP-Contrax.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ai2022whodunit" class="col-sm-8"> <div class="title">Whodunit? Learning to Contrast for Authorship Attribution</div> <div class="author"> <strong>Bo Ai</strong> , Yuchen Wang , Yugin Tan , and <a href="https://samsontmr.github.io/" rel="external nofollow noopener" target="_blank">Samson Tan</a> </div> <div class="periodical"> <em>International Joint Conference on Natural Language Processing (IJCNLP)</em> , 2022 </div> <div class="small-note"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.aacl-main.84" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Authorship attribution is the task of identifying the author of a given text. The key is finding representations that can differentiate between authors. Existing approaches typically use manually designed features that capture a dataset’s content and style, but these approaches are dataset-dependent and yield inconsistent performance across corpora. In this work, we propose \textitlearning author-specific representations by fine-tuning pre-trained generic language representations with a contrastive objective (Contra-X). We show that Contra-X learns representations that form highly separable clusters for different authors. It advances the state-of-the-art on multiple human and machine authorship attribution benchmarks, enabling improvements of up to 6.8% over cross-entropy fine-tuning. However, we find that Contra-X improves overall accuracy at the cost of sacrificing performance for some authors. Resolving this tension will be an important direction for future work. To the best of our knowledge, we are the first to integrate contrastive learning with pre-trained language model fine-tuning for authorship attribution.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ai2022whodunit</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ai, Bo and Wang, Yuchen and Tan, Yugin and Tan, Samson}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{He, Yulan and Ji, Heng and Liu, Yang and Li, Sujian and Chang, Chia{-}Hui and Poria, Soujanya and Lin, Chenghua and Buntine, Wray L. and Liakata, Maria and Yan, Hanqi and Yan, Zonghan and Ruder, Sebastian and Wan, Xiaojun and Arana{-}Catania, Miguel and Wei, Zhongyu and Huang, Hen{-}Hsen and Wu, Jheng{-}Long and Day, Min{-}Yuh and Liu, Pengfei and Xu, Ruifeng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Whodunit? Learning to Contrast for Authorship Attribution}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Joint Conference on Natural Language Processing (IJCNLP)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1142--1157}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.aacl-main.84}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Tue, 29 Nov 2022 14:53:03 +0100}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/ijcnlp/AiWTT22.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Bo Ai </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-YDE6X29CCQ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YDE6X29CCQ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>