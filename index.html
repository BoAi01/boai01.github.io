<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bo Ai </title> <meta name="author" content="Bo Ai"> <meta name="description" content="Bo Ai is an incoming CSE Ph.D. student at UC San Diego, working on robotics and machine learning. He received his Bachelor's degree in CS and Statistics from the National University of Singapore (NUS) and was a visiting researcher intern at the Stanford Vision and Learning Lab (SVL). "> <meta name="keywords" content="robotics, machine learning, artificial intelligence"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ucsd-logo.png?ccd154eecfbaa8871c35216e4979e7b5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://boai01.github.io/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Research </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Bo Ai </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fuji-profile-photo-480.webp 480w,/assets/img/fuji-profile-photo-800.webp 800w,/assets/img/fuji-profile-photo-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/fuji-profile-photo.jpg?b8162d73ef65ed9dff786358287aeed9" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="fuji-profile-photo.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>I am a first-year PhD student in Computer Science and Engineering at <a href="https://ucsd.edu/" rel="external nofollow noopener" target="_blank">UC San Diego</a>, advised by <a href="https://cseweb.ucsd.edu/~haosu/" rel="external nofollow noopener" target="_blank">Hao Su</a>. I am interested in world model learning, large-scale policy learning, and long-horizon skill composition for challenging mobile manipulation tasks.</p> <p>I obtained my Bachelor’s degree with a double major in Computer Science and Statistics from the <a href="https://nus.edu.sg/" rel="external nofollow noopener" target="_blank">National University of Singapore</a> under the Turing Program, where I worked with <a href="https://www.comp.nus.edu.sg/~dyhsu/" rel="external nofollow noopener" target="_blank">David Hsu</a> on long-range navigation using a <a href="https://bostondynamics.com/products/spot/" rel="external nofollow noopener" target="_blank">Spot</a> robot. I was also a visiting student at the <a href="https://svl.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford Vision and Learning Lab</a>, where I worked with <a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a> and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> on learning tactile-informed dynamics models for manipulating complex objects.</p> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%62%61%69@%75%63%73%64.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=KlE77HAAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/BoAi01" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/bo-ai" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/BoAi0110" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">May 05, 2024</th> <td> I will present our work <a href="https://robo-pack.github.io/" rel="external nofollow noopener" target="_blank">RoboPack</a> in three workshops at ICRA 2024: <a href="https://shanluo.github.io/ViTacWorkshops/" rel="external nofollow noopener" target="_blank">ViTac</a>, <a href="https://3d-manipulation-workshop.github.io/" rel="external nofollow noopener" target="_blank">3DVRM</a>, and <a href="https://icra-manipulation-skill.github.io/" rel="external nofollow noopener" target="_blank">Future Roadmap for Manipulation Skills</a>. Welcome to drop by our poster sessions. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 14, 2024</th> <td> I am joining UCSD as a PhD student in 2024 fall! </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024ijrr-kilo-nav-15x-480p-480.webp 480w,/assets/img/publication_preview/2024ijrr-kilo-nav-15x-480p-800.webp 800w,/assets/img/publication_preview/2024ijrr-kilo-nav-15x-480p-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/2024ijrr-kilo-nav-15x-480p.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024ijrr-kilo-nav-15x-480p.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="gao2024intentionnet" class="col-sm-8"> <div class="title">IntentionNet: Map-Lite Visual Navigation at the Kilometre Scale</div> <div class="author"> Wei Gao, <strong>Bo Ai</strong>, Joel Loo,  Vinay, and <a href="https://www.comp.nus.edu.sg/~dyhsu/" rel="external nofollow noopener" target="_blank">David Hsu</a> </div> <div class="periodical"> <em>arXiv</em>, 2024 </div> <div class="small-note"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2407.03122" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This work explores the challenges of creating a scalable and robust robot navigation system that can traverse both indoor and outdoor environments to reach distant goals. We propose a navigation system architecture called IntentionNet that employs a monolithic neural network as the low-level planner/controller, and uses a general interface that we call intentions to steer the controller. The paper proposes two types of intentions, Local Path and Environment (LPE) and Discretised Local Move (DLM), and shows that DLM is robust to significant metric positioning and mapping errors. The paper also presents Kilo-IntentionNet, an instance of the IntentionNet system using the DLM intention that is deployed on a Boston Dynamics Spot robot, and which successfully navigates through complex indoor and outdoor environments over distances of up to a kilometre with only noisy odometry.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024RSS-RoboPack-480.webp 480w,/assets/img/publication_preview/2024RSS-RoboPack-800.webp 800w,/assets/img/publication_preview/2024RSS-RoboPack-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/2024RSS-RoboPack.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024RSS-RoboPack.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ai2024robopack" class="col-sm-8"> <div class="title">RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing</div> <div class="author"> <strong>Bo Ai*</strong>, <a href="https://s-tian.github.io/" rel="external nofollow noopener" target="_blank">Stephen Tian*</a>, <a href="https://hshi74.github.io/" rel="external nofollow noopener" target="_blank">Haochen Shi</a>, <a href="https://wangyixuan12.github.io/" rel="external nofollow noopener" target="_blank">Yixuan Wang</a>, <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan" rel="external nofollow noopener" target="_blank">Cheston Tan</a>, <a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Robotics: Science and Systems (RSS)</em> , 2024 </div> <div class="small-note"> Abridged in ICRA 2024 workshops <a href="https://shanluo.github.io/ViTacWorkshops/" rel="external nofollow noopener" target="_blank">ViTac</a>, <a href="https://3d-manipulation-workshop.github.io/" rel="external nofollow noopener" target="_blank">3DVRM</a>, <a href="https://icra-manipulation-skill.github.io/" rel="external nofollow noopener" target="_blank">Future Roadmap for Sensorimotor Skills</a>, and RSS 2024 workshop <a href="https://sites.google.com/alora.tech/priors4robots24" rel="external nofollow noopener" target="_blank">Priors4Robots</a>. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://robo-pack.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://arxiv.org/abs/2407.01418" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ai2024robopack</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ai*, Bo and Tian*, Stephen and Shi, Haochen and Wang, Yixuan and Tan, Cheston and Li, Yunzhu and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics: Science and Systems (RSS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2407.01418}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Abridged in ICRA 2024 workshops
    [ViTac](https://shanluo.github.io/ViTacWorkshops/),
    [3DVRM](https://3d-manipulation-workshop.github.io/),
    [Future Roadmap for Sensorimotor Skills](https://icra-manipulation-skill.github.io/), and RSS 2024 workshop
    [Priors4Robots](https://sites.google.com/alora.tech/priors4robots24).}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023ISER-SEER-480.webp 480w,/assets/img/publication_preview/2023ISER-SEER-800.webp 800w,/assets/img/publication_preview/2023ISER-SEER-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/2023ISER-SEER.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023ISER-SEER.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ai2023invariance" class="col-sm-8"> <div class="title">Invariance is Key to Generalization: Examining the Role of Representation in Sim-to-Real Transfer for Visual Navigation</div> <div class="author"> <strong>Bo Ai</strong> , <a href="https://zhanxinwu.com/" rel="external nofollow noopener" target="_blank">Zhanxin Wu</a>, and <a href="https://www.comp.nus.edu.sg/~dyhsu/" rel="external nofollow noopener" target="_blank">David Hsu</a> </div> <div class="periodical"> <em>International Symposium on Experimental Robotics (ISER)</em> , 2023 </div> <div class="small-note"> Published within <a href="https://link.springer.com/chapter/10.1007/978-3-031-63596-0_7" rel="external nofollow noopener" target="_blank">Springer Proceedings in Advanced Robotics (SPAR)</a>. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2310.15020" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The data-driven approach to robot control has been gathering pace rapidly, yet generalization to unseen task domains remains a critical challenge. We argue that the key to generalization is representations that are (i) rich enough to capture all task-relevant information and (ii) invariant to superfluous variability between the training and the test domains. We experimentally study such a representation—containing both depth and semantic information—for visual navigation and show that it enables a control policy trained entirely in simulated indoor scenes to generalize to diverse real-world environments, both indoors and outdoors. Further, we show that our representation reduces the A-distance between the training and test domains, improving the generalization error bound as a result. Our proposed approach is scalable: the learned policy improves continuously, as the foundation models that it exploits absorb more diverse data during pre-training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2022ICRA-DECISION-480.webp 480w,/assets/img/publication_preview/2022ICRA-DECISION-800.webp 800w,/assets/img/publication_preview/2022ICRA-DECISION-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/2022ICRA-DECISION.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022ICRA-DECISION.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ai2022deep" class="col-sm-8"> <div class="title">Deep Visual Navigation under Partial Observability</div> <div class="author"> <strong>Bo Ai</strong>, Wei Gao,  Vinay, and <a href="https://www.comp.nus.edu.sg/~dyhsu/" rel="external nofollow noopener" target="_blank">David Hsu</a> </div> <div class="periodical"> <em>International Conference on Robotics and Automation (ICRA)</em> , 2022 </div> <div class="small-note"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://adacomp.comp.nus.edu.sg/inet/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://arxiv.org/abs/2109.07752" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>How can a robot navigate successfully in rich and diverse environments, indoors or outdoors, along office corridors or trails on the grassland, on the flat ground or the staircase? To this end, this work aims to address three challenges: (i) complex visual observations, (ii) partial observability of local visual sensing, and (iii) multimodal robot behaviors conditioned on both the local environment and the global navigation objective. We propose to train a neural network (NN) controller for local navigation via imitation learning. To tackle complex visual observations, we extract multi-scale spatial representations through CNNs. To tackle partial observability, we aggregate multi-scale spatial information over time and encode it in LSTMs. To learn multimodal behaviors, we use a separate memory module for each behavior mode. Importantly, we integrate the multiple neural network modules into a unified controller that achieves robust performance for visual navigation in complex, partially observable environments. We implemented the controller on the quadrupedal Spot robot and evaluated it on three challenging tasks: adversarial pedestrian avoidance, blind-spot obstacle avoidance, and elevator riding. The experiments show that the proposed NN architecture significantly improves navigation performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ai2022deep</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ai, Bo and Gao, Wei and Vinay and Hsu, David}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Visual Navigation under Partial Observability}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9439--9446}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICRA46639.2022.9811598}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA46639.2022.9811598}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Mon, 04 Dec 2023 21:29:46 +0100}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/icra/AiGVH22.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-note">"We shall not cease from exploration. <br> And the end of all our exploring will be to arrive where we started, and know the place for the first time." <br> — T. S. Eliot </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Bo Ai </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-YDE6X29CCQ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YDE6X29CCQ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>